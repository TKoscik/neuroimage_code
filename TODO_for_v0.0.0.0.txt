# TO DO =======================================================================
## Version: 0.0.0.0
## Updated: 2020-11-16, 2021-06-25, 2021-09-07

[ ] check help for all files, update with only used and update arguments
[ ] remove incomplete functions, keep for copy to dev branch after versioning
[ ] update HCP templates (decide if MNI variant)
[ ] add in ${DIR_TMP} for top level of scratch directory
[ ] finish and debug make_template.sh
[ ] check/debug T1rho function
[ ] check that diffusion tractography is using the methods we prefer
[ ] correlation matrix to long output, with from and to columns, maybe a summary function to pull seed to seed connectivity

# functional workflow ---------------------------------------------------------
[ ] ts_deconvolve.sh
    - simultaneously to nuisance_regression? or does that interfere with LSS?
    [ ] LSS and LSA options (LSS as default)
    [ ] allow amplitude and duration modulation with stim_times_IM, LSS?
    [ ] provide options for HRF, but default to canonical
[ ] connectivity_mx.sh, output of roi_ts.sh (using R would be easy)
    [ ] optional correlation coefficient or z-score
    [ ] optional cross-correlation order

## NOTES: to be implemented
Logging: tsv files for logging
  [ ] a subfolder for benchmarking: /Dedicated/inc_database/log/benchmark
      [ ] track resource usage and timing whenever most functions of ours are called
          - useful for future cost/computing needs estimates
      [ ] filenames would be the function name: /Dedicated/inc_database/log/benchmark/fcn-${FCN_NAME}_date-FY20##Q#.tsv
          [ ] it would make things slightly easier if we took underscores out of function names... i'll have a think on this.
      [ ] columns: operator, hardware, kernel, hpc queue, hpc slots, start time, end time, exit code, maybe error description if possible?
  [ ] a subfolder for QC tracking: /Dedicated/inc_database/log/qc
      [ ] tracking users and timing, so I can provide feedback on meeting processing goals (i.e., I really want to make a push to get us to the on demand state without a backlog)
          -filenames would reflect the qc step: e.g.,  /Dedicated/inc_database/log/qc/qc-dicomConversion_date-FY20##Q#.tsv
      [ ] columns: operator, scan date/time, process available time (earliest possible time this step became available), qc start date/time, qc complete date/time
  [ ] a subfolder for project tracking: /Dedicated/inc_database/log/project
      two files:
        [ ] /Dedicated/inc_database/log/project/pi-${PI}_project-{$PROJECT}_log-process.tsv
            [ ] log everything that is run, maybe a bit verbose for the change.log but the same idea
            [ ] columns for PID & SID (participant and session id's), item description (e.g., scan acquisition, function name that is run, etc.), start time for process, end time of process, resources used (argon queue name and number of slots), operator,
            [ ] can add group analysis processes in here as well and just have GROUP for the PID and
        [ ] /Dedicated/inc_database/log/project/pi-${PI}_project-${PROJECT}_log-status.tsv
            [ ] This file is more subject centred to make sure we are doing the things we want for each project, and could be used to track what has been done for each participant and what remains to be done
            [ ] columns would be: PID, SID, acquisition_date, <columns for each process, populated with dates as they are finished>






